{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import gzip\n",
    "from io import StringIO;\n",
    "import os\n",
    "\n",
    "import torch \n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import seaborn as sns \n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import gzip\n",
    "import shutil\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"ExtraSensory.per_uuid_features_labels/\"\n",
    "def parse_header_of_csv(csv_str):\n",
    "    # Isolate the headline columns:\n",
    "    headline = csv_str[:csv_str.index(b'\\n')];\n",
    "    columns = headline.split(b',');\n",
    "\n",
    "    # The first column should be timestamp:\n",
    "    assert columns[0] == b'timestamp';\n",
    "    \n",
    "    # The last column should be label_source:\n",
    "    assert columns[-1] == b'label_source';\n",
    "    \n",
    "    # Search for the column of the first label:\n",
    "    for (ci,col) in enumerate(columns):\n",
    "        if col.startswith(b'label:'):\n",
    "            first_label_ind = ci;\n",
    "            break;\n",
    "        pass;\n",
    "\n",
    "    # Feature columns come after timestamp and before the labels:\n",
    "    feature_names = columns[1:first_label_ind];\n",
    "    \n",
    "    # Then come the labels, till the one-before-last column:\n",
    "    label_names = columns[first_label_ind:-1];\n",
    "    for (li,label) in enumerate(label_names):\n",
    "        \n",
    "        # In the CSV the label names appear with prefix 'label:', but we don't need it after reading the data:\n",
    "        assert label.startswith(b'label:');\n",
    "        label_names[li] = label.replace(b'label:',b'');\n",
    "        pass;\n",
    "    \n",
    "    return (feature_names, label_names);\n",
    "\n",
    "def parse_body_of_csv(csv_str,n_features):\n",
    "    # Read the entire CSV body into a single numeric matrix:\n",
    "    full_table = np.loadtxt(StringIO(csv_str.decode(\"utf-8\")),delimiter=',',skiprows=1);\n",
    "    \n",
    "    # Timestamp is the primary key for the records (examples):\n",
    "    timestamps = full_table[:,0].astype(int);\n",
    "    \n",
    "    # Read the sensor features:\n",
    "    X = full_table[:,1:(n_features+1)];\n",
    "    \n",
    "    # Read the binary label values, and the 'missing label' indicators:\n",
    "    trinary_labels_mat = full_table[:,(n_features+1):-1]; # This should have values of either 0., 1. or NaN\n",
    "    M = np.isnan(trinary_labels_mat); # M is the missing label matrix\n",
    "    Y = np.where(M,0,trinary_labels_mat) > 0.; # Y is the label matrix\n",
    "    \n",
    "    return (X,Y,M,timestamps);\n",
    "\n",
    "'''\n",
    "Read the data (precomputed sensor-features and labels) for a user.\n",
    "This function assumes the user's data file is present.\n",
    "'''\n",
    "def read_user_data(uuid):\n",
    "    user_data_file = '%s%s.features_labels.csv.gz' % (root_dir, uuid);\n",
    "\n",
    "    # Read the entire csv file of the user:\n",
    "    with gzip.open(user_data_file,'rb') as fid:\n",
    "        csv_str = fid.read();\n",
    "        pass;\n",
    "    \n",
    "    (feature_names,label_names) = parse_header_of_csv(csv_str);\n",
    "    n_features = len(feature_names);\n",
    "    (X,Y,M,timestamps) = parse_body_of_csv(csv_str,n_features);\n",
    "\n",
    "    return (X,Y,M,timestamps,feature_names,label_names);\n",
    "\n",
    "onlyfiles = [f for f in listdir(root_dir)]\n",
    "final_files = [] \n",
    "for i in onlyfiles: \n",
    "    #x = '/%s%s' % (root_dir, i[0:len(i)-3]) \n",
    "    if \".csv\" in i and \".gz\" not in i:\n",
    "        final_files.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_d = pd.read_csv('./'+root_dir + final_files[0] )\n",
    "sensor_data = raw_d.iloc[:,1:225]\n",
    "labels_data = raw_d.iloc[:,226:-1]\n",
    "\n",
    "for i in range(1,len(final_files)):\n",
    "    raw_d = pd.read_csv('./'+root_dir + final_files[i] )\n",
    "    sensor_data = sensor_data.append(raw_d.iloc[:,1:225],ignore_index=True)\n",
    "    labels_data = labels_data.append(raw_d.iloc[:,226:-1],ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_data = sensor_data.fillna(0)\n",
    "labels_data = labels_data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_col_mean = []\n",
    "per_col_std = []\n",
    "def normalize_data(data):\n",
    "    norm_data = pd.DataFrame() \n",
    "    for col in data.columns: \n",
    "        col_mean = data[col].mean()\n",
    "        col_std = data[col].std()\n",
    "        per_col_mean.append(col_mean)\n",
    "        per_col_std.append(col_std)\n",
    "        if col_std != 0:\n",
    "            norm_data[col] = (data[col] - col_mean)/col_std\n",
    "        else:\n",
    "            norm_data[col] = data[col]    \n",
    "    #norm_data.dropna(inplace = True, axis = 'columns' )\n",
    "    return norm_data\n",
    "def normalize_test_data(data):\n",
    "    norm_data = pd.DataFrame() \n",
    "    i  = 0\n",
    "    for col in data.columns: \n",
    "        col_mean = per_col_mean[i]\n",
    "        col_std = per_col_std[i]\n",
    "        if col_std != 0:\n",
    "            norm_data[col] = (data[col] - col_mean)/col_std\n",
    "        else:\n",
    "            norm_data[col] = data[col]   \n",
    "        i = i +1\n",
    "    \n",
    "    #norm_data.dropna(inplace = True, axis = 'columns' )\n",
    "    return norm_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1346578)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "377346"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sensor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_label_index = []\n",
    "for i in range(len(labels_data)):\n",
    "    s = labels_data.loc[i,:].sum()\n",
    "    if s ==0: \n",
    "        empty_label_index.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326687\n"
     ]
    }
   ],
   "source": [
    "labels_data = labels_data.drop(empty_label_index)\n",
    "sensor_data = sensor_data.drop(empty_label_index)\n",
    "print(len(sensor_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty labels percentage: 13.425079370127147  %\n"
     ]
    }
   ],
   "source": [
    "print(\"Empty labels percentage:\",(1-326687/377346)*100,\" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(sensor_data)\n",
    "all_indices = [i for i in range(dataset_size)]\n",
    "# random.shuffle(all_indices) #pick random trainig samples\n",
    "row_train = all_indices[:int(2*dataset_size/3)]\n",
    "row_test = all_indices[int(2*dataset_size/3):]\n",
    "\n",
    "train_data = sensor_data.iloc[row_train,:]\n",
    "test_data = sensor_data.iloc[row_test,:]\n",
    "train_data = normalize_data(train_data)\n",
    "test_data = normalize_test_data(test_data)\n",
    "\n",
    "labels_train=labels_data.iloc[row_train,:]\n",
    "labels_test=labels_data.iloc[row_test,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_sens_data_set = {} \n",
    "extra_sens_data_set[\"X_train\"] = train_data.to_numpy(dtype=np.float32)\n",
    "extra_sens_data_set[\"X_test\"] = test_data.to_numpy(dtype=np.float32)\n",
    "extra_sens_data_set[\"Y_train\"] = labels_train.to_numpy(dtype=np.float32)\n",
    "extra_sens_data_set[\"Y_test\"] = labels_test.to_numpy(dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./extra_sens_data_set.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(extra_sens_data_set,f)\n",
    "\n",
    "input_shape = 224\n",
    "input_shape_s1 = 14\n",
    "input_shape_s2 = 16\n",
    "class SensorDataset(Dataset): \n",
    "    \n",
    "    def __init__(self, sensor_data, transform,labels_data):\n",
    "        \"\"\"provide directory with the user's data\"\"\"\n",
    "        self.sensor_data = sensor_data\n",
    "        self.transform = transform\n",
    "        self.labels_data = labels_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sensor_data)\n",
    "        \n",
    "    # This function can be used to index into the dataset\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "                 \n",
    "\n",
    "        sample =  self.sensor_data[idx,:]\n",
    "        labels =  self.labels_data[idx,:]\n",
    "        \n",
    "        if self.transform:\n",
    "            # Transform to tensor\n",
    "            sample = self.transform(sample.reshape(input_shape_s1,input_shape_s2))\n",
    "            \n",
    "        return sample, torch.from_numpy(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_sets(esds):\n",
    "    img_transform = transforms.Compose([\n",
    "        transforms.ToTensor()])\n",
    "    \n",
    "    train_data = SensorDataset(esds[\"X_train\"], transform = img_transform,labels_data=esds[\"Y_train\"])\n",
    "    test_data = SensorDataset(esds[\"X_test\"], transform = img_transform,labels_data=esds[\"Y_test\"])\n",
    "    \n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size = 1, shuffle = False)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size = 1, shuffle=False)\n",
    "    \n",
    "    return train_data, test_data, train_loader, test_loader\n",
    "train_data, test_data, train_loader, test_loader = create_train_test_sets(esds=extra_sens_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw = torch.zeros(len(train_loader),1,224)\n",
    "X_test_raw = torch.zeros(len(test_loader),1,224)\n",
    "\n",
    "Y_train_encd = torch.zeros(len(train_loader),1,51)\n",
    "Y_test_encd = torch.zeros(len(test_loader),1,51)\n",
    "\n",
    "def generate_featuers():\n",
    "    with torch.no_grad():\n",
    "        cnt = 0\n",
    "\n",
    "        # Iterate through the test dataset (we are using this data for validation, too)\n",
    "        for batch_features, y_obs in train_loader:\n",
    "            # Reshape mini-batch data to [N, input_shape] matrix\n",
    "            batch_features = batch_features.view(-1, input_shape)\n",
    "            # Use the model\n",
    "\n",
    "            X_train_raw[cnt,:,:] =batch_features\n",
    "            Y_train_encd[cnt,:,:]=y_obs\n",
    "            cnt+=1\n",
    "            \n",
    "        cnt = 0\n",
    "        # Iterate through the test dataset (we are using this data for validation, too)\n",
    "        for batch_features, y_obs in test_loader:\n",
    "            # Reshape mini-batch data to [N, input_shape] matrix\n",
    "            batch_features = batch_features.view(-1, input_shape)\n",
    "            # Use the model\n",
    "            X_test_raw[cnt,:,:] =batch_features\n",
    "            Y_test_encd[cnt,:,:]=y_obs\n",
    "            cnt+=1\n",
    "            \n",
    "generate_featuers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_sens_data_set_with_features = {} \n",
    "\n",
    "extra_sens_data_set_with_features[\"X_train\"] = X_train_raw\n",
    "extra_sens_data_set_with_features[\"X_test\"] = X_test_raw\n",
    "extra_sens_data_set_with_features[\"Y_train\"] = Y_train_encd\n",
    "extra_sens_data_set_with_features[\"Y_test\"] = Y_test_encd\n",
    "\n",
    "with open(\"./extra_sens_data_set_with_features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(extra_sens_data_set_with_features,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
